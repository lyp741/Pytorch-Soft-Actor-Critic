{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txtyJlCt0Rly",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!killall 9-swarm.x86_64\n",
    "from mlagents.envs import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnI3nIje0Rl4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vv8Kx-8x0Rl7"
   },
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uf6gPWP80Rl8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4X3jRQD0Rl-"
   },
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGDk4qTZ0Rl_",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.batch_size = batch_size\n",
    "        self.position = 0\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KieKl4E0RmB",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def _action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer2:\n",
    "    \" Internal memory of the agent \"\n",
    "    \n",
    "    def __init__(self, buffer_size, batch_size, n_agents=1, seed=0):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \" Add a new experience to memory \"\n",
    "        for i in range(self.n_agents):\n",
    "            e = self.experience(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "            self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \" Randomly sample a batch of experiences from the memory \"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "#         print(dones.shape)\n",
    "#         dones = done.squeeze(axis = 1)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \" Return the current size of internal memory. Overwrites the inherited function len. \"\n",
    "        \n",
    "        return len(self.memory)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynjwRBCl0RmD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dec_f_go0RmG"
   },
   "source": [
    "<h1>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1801.01290\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPAkyjg80RmH",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, 400)\n",
    "        self.linear2 = nn.Linear(400, 300)\n",
    "        self.linear3 = nn.Linear(300, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, 400)\n",
    "        self.linear2 = nn.Linear(400, 300)\n",
    "        self.linear3 = nn.Linear(300, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, 400)\n",
    "        self.linear2 = nn.Linear(400, 300)\n",
    "        \n",
    "        self.mean_linear = nn.Linear(300, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(300, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "#         action = mean\n",
    "        action  = action.detach().cpu().numpy()\n",
    "#         print(action)\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdT8llxi0RmJ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def soft_q_update(batch_size, \n",
    "           gamma=0.99,\n",
    "           mean_lambda=1e-3,\n",
    "           std_lambda=1e-3,\n",
    "           z_lambda=0.0,\n",
    "           soft_tau=1e-2,\n",
    "          ):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample()\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    expected_q_value = soft_q_net(state, action)\n",
    "    expected_value   = value_net(state)\n",
    "    new_action, log_prob, z, mean, log_std = policy_net.evaluate(state)\n",
    "\n",
    "\n",
    "    target_value = target_value_net(next_state)\n",
    "    next_q_value = reward + (1 - done) * gamma * target_value\n",
    "    q_value_loss = soft_q_criterion(expected_q_value, next_q_value.detach())\n",
    "\n",
    "    expected_new_q_value = soft_q_net(state, new_action)\n",
    "    next_value = expected_new_q_value - log_prob\n",
    "    value_loss = value_criterion(expected_value, next_value.detach())\n",
    "\n",
    "    log_prob_target = expected_new_q_value - expected_value\n",
    "    policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "#     print(policy_loss.item(),log_prob[0],expected_new_q_value[0])\n",
    "\n",
    "    mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "    std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "    z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "\n",
    "    policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "    soft_q_optimizer.zero_grad()\n",
    "    q_value_loss.backward()\n",
    "    soft_q_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )\n",
    "    return policy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_q_update1(batch_size):\n",
    "    gamma=0.99\n",
    "    soft_tau=1e-2\n",
    "    alpha = 0.2\n",
    "    mean_lambda=1e-3\n",
    "    std_lambda=1e-3\n",
    "    z_lambda=0.0\n",
    "    state, action, reward, next_state, done = replay_buffer.sample()\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).to(device)\n",
    "    new_action, log_prob, z, mean, log_std = policy_net.evaluate(state)\n",
    "    v_target = target_value_net(next_state)\n",
    "    q1 = soft_q_net(state, action)\n",
    "    q2 = soft_q2_net(state, action)\n",
    "    v = value_net(state)\n",
    "    q_bak = (reward + (1 - done) * gamma * v_target).detach()\n",
    "    q1_pi = soft_q_net(state, new_action)\n",
    "    q2_pi = soft_q2_net(state, new_action)\n",
    "    min_q_pi = torch.min(q1_pi, q2_pi)\n",
    "    \n",
    "    v_bak = (min_q_pi - alpha*log_prob).detach()\n",
    "    pi_loss = (alpha*log_prob - min_q_pi).mean()\n",
    "    q1_loss = 0.5*F.mse_loss(q1,q_bak)\n",
    "    q2_loss = 0.5*F.mse_loss(q2,q_bak)\n",
    "    v_loss = 0.5*F.mse_loss(v,v_bak)\n",
    "    \n",
    "    \n",
    "    log_prob_target = q1_pi - v\n",
    "    policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "\n",
    "    mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "    std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "    z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "    \n",
    "    policy_loss += mean_loss + std_loss + z_loss\n",
    "    policy_optimizer.zero_grad()\n",
    "    pi_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    soft_q_optimizer.zero_grad()\n",
    "    q1_loss.backward()\n",
    "    soft_q_optimizer.step()\n",
    "    soft_q2_optimizer.zero_grad()\n",
    "    q2_loss.backward()\n",
    "    soft_q2_optimizer.step()\n",
    "    value_optimizer.zero_grad()\n",
    "    v_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )\n",
    "    return v_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_q_update2(batch_size):\n",
    "    gamma=0.99\n",
    "    soft_tau=1e-2\n",
    "    alpha = 0.2\n",
    "    mean_lambda=1e-3\n",
    "    std_lambda=1e-3\n",
    "    z_lambda=0.0\n",
    "    state, action, reward, next_state, done = replay_buffer.sample()\n",
    "    state      = torch.FloatTensor(state).squeeze(1).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).squeeze(1).to(device)\n",
    "    action     = torch.FloatTensor(action).squeeze(1).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).to(device)\n",
    "#     print(state.shape, action.shape, next_state.shape, done.shape)\n",
    "#     print(1 - done)\n",
    "    with torch.no_grad():\n",
    "        v_target = target_value_net(next_state)\n",
    "        next_q_value = reward + (1 - done) * gamma * (v_target)\n",
    "#         print('next q: ',next_q_value)\n",
    "    q1 = soft_q_net(state, action)\n",
    "    q1_loss = F.mse_loss(q1, next_q_value)\n",
    "    q2 = soft_q2_net(state, action)\n",
    "    q2_loss = F.mse_loss(q2, next_q_value)\n",
    "    new_action, log_prob, z, mean, log_std = policy_net.evaluate(state)\n",
    "    q1_pi = soft_q_net(state, new_action)\n",
    "    q2_pi = soft_q2_net(state, new_action)\n",
    "    min_q_pi = torch.min(q1_pi,q2_pi)\n",
    "    policy_loss = ((alpha * log_prob) - min_q_pi).mean()\n",
    "#     reg_loss = 0.001 * (mean.pow(2).mean() + log_std.pow(2).mean())\n",
    "#     policy_loss += reg_loss\n",
    "    \n",
    "    vf = value_net(state)\n",
    "    with torch.no_grad():\n",
    "        vf_target = min_q_pi - (alpha * log_prob)\n",
    "    \n",
    "    vf_loss = F.mse_loss(vf, vf_target)\n",
    "    \n",
    "    \n",
    "#     log_prob_target = q1_pi - vf\n",
    "#     policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "\n",
    "#     mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "#     std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "#     z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "    \n",
    "#     policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "    soft_q_optimizer.zero_grad()\n",
    "    q1_loss.backward()\n",
    "    soft_q_optimizer.step()\n",
    "    soft_q2_optimizer.zero_grad()\n",
    "    q2_loss.backward()\n",
    "    soft_q2_optimizer.step()\n",
    "    value_optimizer.zero_grad()\n",
    "    vf_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "#     soft_q2_optimizer.zero_grad()\n",
    "#     q2_loss.backward()\n",
    "#     soft_q2_optimizer.step()\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )\n",
    "    return policy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlagents.envs import UnityEnvironment\n",
    "import numpy as np\n",
    "env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
    "# env = UnityEnvironment(file_name = \\\n",
    "#                        '/home/ohk/Downloads/ContinuousControl-D4PG-master/Reacher_Linux/9-swarm.x86_64',\n",
    "#                       no_graphics=False,base_port=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]\n",
    "# print(brain_name)\n",
    "# print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # reset the environment\n",
    "# print('reset the environment')\n",
    "# env_info = env.reset(train_mode=True)[brain_name]\n",
    "# print('finished reset')\n",
    "# # number of agents\n",
    "# num_agents = len(env_info.agents)\n",
    "# print('Number of agents:', num_agents)\n",
    "\n",
    "# # size of each action\n",
    "# action_dim = brain.vector_action_space_size[0]\n",
    "# print('Size of each action:', action_dim)\n",
    "\n",
    "# # examine the state space \n",
    "# states = env_info.vector_observations\n",
    "# state_dim = states.shape[1] #ndarray\n",
    "# print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], states.shape))\n",
    "# print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.reset()\n",
    "num_agents = 1\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "if( finished ):\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0shUhdEW0RmR",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_frames  = 40000\n",
    "max_steps   = 500\n",
    "frame_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 128\n",
    "p_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aMEanpTP0RmM",
    "outputId": "676b2cb4-eca8-4729-fdad-428b033e1e0e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net        = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "target_value_net = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "\n",
    "soft_q_net = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "soft_q2_net = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion = nn.MSELoss()\n",
    "\n",
    "value_lr  = 1e-2\n",
    "\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "soft_q_optimizer = optim.Adam(soft_q_net.parameters(), lr=value_lr)\n",
    "soft_q2_optimizer = optim.Adam(soft_q2_net.parameters(), lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=value_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5smGTbtX0RmS",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "max_frames  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHA_aYX90RmV",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# while frame_idx < max_frames:\n",
    "#     state = env.reset()\n",
    "#     episode_reward = 0\n",
    "#     \n",
    "#     for step in range(max_steps):\n",
    "#         action = policy_net.get_action(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "# #         env.render()\n",
    "#         replay_buffer.push(state, action, reward, next_state, done)\n",
    "#         if len(replay_buffer) > batch_size:\n",
    "#             l = soft_q_update(batch_size)\n",
    "#             p_l.append(l)\n",
    "#         \n",
    "#         state = next_state\n",
    "#         episode_reward += reward\n",
    "#         frame_idx += 1\n",
    "#         \n",
    "#         if frame_idx % 1000 == 0:\n",
    "# #             plot(frame_idx, rewards)\n",
    "#             plot(frame_idx, p_l)\n",
    "#             print(episode_reward)\n",
    "#         if done:\n",
    "#             break\n",
    "#         \n",
    "#     rewards.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NormalizedActions<TimeLimit<PendulumEnv<Pendulum-v0>>>>\n"
     ]
    }
   ],
   "source": [
    "print(env)\n",
    "scores = []\n",
    "policy_loss = []\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 1.7688100337982178\n",
      "episode  36  start at  2019-05-21 21:07:27.633801\n",
      "36 1.6902875900268555\n",
      "episode  37  start at  2019-05-21 21:07:29.324154\n",
      "37 1.700530767440796\n",
      "episode  38  start at  2019-05-21 21:07:31.024765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-54cc8e449394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                     print('state ',i,state_i.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[brain_name]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9e2b1452826c>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9e2b1452826c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmean\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestep = 0\n",
    "rewards = []\n",
    "for i_episode in range(1, 100):\n",
    "    st = time.time()\n",
    "    episode_r = 0\n",
    "    print('episode ',i_episode,' start at ',datetime.datetime.now())\n",
    "#     env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "#     state = env_info.vector_observations\n",
    "    state = np.array([env.reset()])\n",
    "#     print(state)\n",
    "    score = np.zeros(num_agents)\n",
    "    for t in range(5000):\n",
    "        action = np.zeros((num_agents, action_dim)) # initialize multidimensional array actions of each agent\n",
    "        for i in range(num_agents):\n",
    "                state_i = torch.from_numpy(state[i,:]).float()\n",
    "                with torch.no_grad():\n",
    "#                     print('state ',i,state_i.shape)\n",
    "                    action[i,:] =  policy_net.get_action(state_i)\n",
    "#         print(action)\n",
    "        env_info = env.step(action[0])#[brain_name]\n",
    "#         print(env_info)\n",
    "#         next_state = np.array([np.transpose(env_info[0])])\n",
    "#         next_state = np.array([[env_info[0]]])\n",
    "#         reward = np.array([env_info[1]])                        # get reward (for each agent)\n",
    "#         done = np.array([env_info[2]])\n",
    "        next_state = np.array([env_info[0]])\n",
    "#         print('next_', next_state)\n",
    "        reward = env_info[1]\n",
    "        done = env_info[2]\n",
    "        \n",
    "        score += reward\n",
    "#         print(env_info)\n",
    "#         print('add ', state, state.shape)\n",
    "#         print('done',done.shape)\n",
    "#         print(state, action, reward, next_state, done)\n",
    "#         print(state)\n",
    "        replay_buffer.add(state,action,reward,next_state,done)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            policy_l = soft_q_update2(batch_size)\n",
    "            policy_loss.append(policy_l)\n",
    "        state = next_state\n",
    "        timestep += 1\n",
    "        episode_r += np.sum(reward)\n",
    "        \n",
    "        if timestep % 1000 == 0:\n",
    "            plot(timestep, rewards)\n",
    "        if np.any(done):\n",
    "            break\n",
    "    rewards.append(episode_r)\n",
    "    print(i_episode,time.time()-st)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(i_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "7.soft actor-critic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "PyCharm (untitled)",
   "language": "python",
   "name": "pycharm-8c077bca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
